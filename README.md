# KSU_Thesis


# Using Big Data Analytics to Optimize Practical Large Databases


# Abstract: 

Huge data management has become a hot topic, trying to find a feasible method to manage and optimize large natural language database. Including data preprocessing and analysis, and the feasibility of converting logical representations.

# Source:

IT company in U.S.A Main business, assisting customers with hardware and software issues. Example: When the customer encounters that the application data cannot be stored, report it to the company, and the company searches the self-repository and returns the solution. Both the problem description and the database are in English (natural language description).

# Discuss ideas: 

Due to too many natural languages in the database, as well as the fallacies and vague descriptions caused by natural language, when the database becomes larger and larger, the efficiency will be greatly reduced, and the errors of the database may be unpredictable and maintainable.

# Discussion method: 

Use python NLP to preprocess the data with obvious errors and garbled characters in the data, and then convert the data into logical representation. According to the literature, it is possible to increase the performance.

# Conclusion:

The feasibility of converting natural language databases into logical representations is established.

# Problem discussion: 

Although it is feasible to establish such an open source data conversion system, in order to meet the input formats of different conversion systems, it will lead to a longer process and involve too many human efforts, which is not in line with economic interests.

# Problem discussion: 

The huge natural language database has been established for a long time. Because of the different language habits used by different agents, the pre-processing of the data will also be complicated in the past, and the data and conversion systems need to be further optimized.
